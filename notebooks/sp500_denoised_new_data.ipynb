{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from datetime import time\n",
    "from pytz import timezone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomyślnie załadowano i przetworzono plik: SPX_2021_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) —— Data Import & Preprocessing —— #\n",
    "\n",
    "DATA_DIR = \"../SPX\"\n",
    "DATA_DIR_new = \"../SPX_new\"\n",
    "FEATURE_CSV = \"../csvfiles_new/features_360.csv\"\n",
    "ALLSET_CSV  = \"../csvfiles_new/allSet_360.csv\"\n",
    "\n",
    "def load_data(path):\n",
    "    files = glob.glob(os.path.join(path, \"*.txt\"))\n",
    "    dfs = []\n",
    "    for f in sorted(files):\n",
    "        df = pd.read_csv(f, names=[\"DateTime\",\"Open\",\"High\",\"Low\",\"Close\"],\n",
    "                         sep=\",\", parse_dates=[\"DateTime\"])\n",
    "        dfs.append(df)\n",
    "    spx = pd.concat(dfs, ignore_index=True)\n",
    "    # localize to EST\n",
    "    spx[\"DateTime\"] = spx[\"DateTime\"].dt.tz_localize(timezone(\"US/Eastern\"))\n",
    "    spx.set_index(\"DateTime\", inplace=True)\n",
    "    #Keep dates up until 2020\n",
    "    spx = spx.loc[:'2020']\n",
    "    # filter trading hours 9:30–16:00\n",
    "    return spx.between_time(\"09:30\",\"16:00\")\n",
    "\n",
    "spx = load_data(DATA_DIR)\n",
    "\n",
    "def load_data_new(path):\n",
    "    file_name = \"SPX_2021_2025.csv\"\n",
    "    full_file_path = os.path.join(path, file_name)\n",
    "    spx_new = pd.read_csv(full_file_path, \n",
    "                     names=[\"DateTime\", \"Open\", \"High\", \"Low\", \"Close\"],\n",
    "                     sep=\",\", \n",
    "                     parse_dates=[\"DateTime\"],\n",
    "                     header=0)\n",
    "    # Set dates to datetime\n",
    "    spx_new[\"DateTime\"] = pd.to_datetime(spx_new[\"DateTime\"], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    # localize to EST\n",
    "    spx_new[\"DateTime\"] = spx_new[\"DateTime\"].dt.tz_localize(timezone(\"US/Eastern\"))\n",
    "    spx_new.set_index(\"DateTime\", inplace=True)\n",
    "    # filter trading hours 9:30–16:00\n",
    "    spx_new = spx_new.between_time(\"09:30\", \"16:00\")\n",
    "\n",
    "    print(f\"Pomyślnie załadowano i przetworzono plik: {file_name}\")\n",
    "    return spx_new\n",
    "\n",
    "spx_new = load_data_new(DATA_DIR_new)\n",
    "\n",
    "# Drop duplicates\n",
    "spx = spx[~spx.index.duplicated(keep='first')]\n",
    "\n",
    "spx = pd.concat([spx, spx_new], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a) —— Discrete Wavelet Shrinkage on each price series —— #\n",
    "\n",
    "def wav_shrink(x, wavelet=\"db2\", level=1, mode=\"soft\"):\n",
    "    \"\"\"\n",
    "    Decompose x via DWT, apply universal soft-threshold to detail coeffs,\n",
    "    and reconstruct.\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(x, wavelet, level=level)\n",
    "    # estimate noise sigma from finest detail coeffs\n",
    "    detail = coeffs[-1]\n",
    "    sigma = np.median(np.abs(detail)) / 0.6745\n",
    "    # universal threshold\n",
    "    thr = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "    # threshold detail coefficients only\n",
    "    coeffs_shrunk = [coeffs[0]] + [pywt.threshold(c, thr, mode=mode) for c in coeffs[1:]]\n",
    "    return pywt.waverec(coeffs_shrunk, wavelet)[: len(x)]\n",
    "\n",
    "# apply to each column, keep pandas Series with same index\n",
    "dwt_Close = pd.Series(\n",
    "    wav_shrink(spx[\"Close\"].values), index=spx.index, name=\"Close\")\n",
    "dwt_High  = pd.Series(\n",
    "    wav_shrink(spx[\"High\"].values),  index=spx.index, name=\"High\")\n",
    "dwt_Low   = pd.Series(\n",
    "    wav_shrink(spx[\"Low\"].values),   index=spx.index, name=\"Low\")\n",
    "dwt_Open  = pd.Series(\n",
    "    wav_shrink(spx[\"Open\"].values),  index=spx.index, name=\"Open\")\n",
    "\n",
    "# override spx with the denoised (DWT) series\n",
    "spx_dwt = spx.copy()\n",
    "for col, series in [(\"High\", dwt_High), (\"Low\", dwt_Low),\n",
    "                    (\"Close\", dwt_Close), (\"Open\", dwt_Open)]:\n",
    "    spx_dwt[col] = series\n",
    "spx = spx_dwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157652/2650437742.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  avgP = groups.apply(lambda df: df[\"Close\"].iloc[:-30].mean())\n"
     ]
    }
   ],
   "source": [
    "# 2) —— Create daily labels based on DWT-filtered Close —— #\n",
    "\n",
    "spx[\"Date\"] = spx.index.date\n",
    "groups = spx.groupby(\"Date\")\n",
    "\n",
    "# last-minute dwt_Close each day\n",
    "lmP = groups[\"Close\"].last()\n",
    "\n",
    "# average from first bar up to bar-30 (i.e. minus 30 minutes)\n",
    "avgP = groups.apply(lambda df: df[\"Close\"].iloc[:-30].mean())\n",
    "\n",
    "y = (avgP < lmP).astype(int)\n",
    "y.name = \"Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) —— Indicator definitions (copied from first translation) —— #\n",
    "\n",
    "def SMA(x, n=20):\n",
    "    return x.rolling(n, min_periods=n).mean()\n",
    "\n",
    "def EMA(x, n=20):\n",
    "    return x.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def DEMA(x, n=20):\n",
    "    e = EMA(x, n)\n",
    "    return 2*e - EMA(e, n)\n",
    "\n",
    "def ATR(H, L, C, n=14):\n",
    "    prevC = C.shift(1)\n",
    "    tr = pd.concat([\n",
    "        H - L,\n",
    "        (H - prevC).abs(),\n",
    "        (L - prevC).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return EMA(tr, n)\n",
    "\n",
    "def ADX(H, L, C, n=14):\n",
    "    up = H.diff()\n",
    "    down = -L.diff()\n",
    "    plusDM  = np.where((up>down)&(up>0), up, 0.0)\n",
    "    minusDM = np.where((down>up)&(down>0), down, 0.0)\n",
    "    tr = pd.concat([\n",
    "        H-L,\n",
    "        (H - C.shift(1)).abs(),\n",
    "        (L - C.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    atr = EMA(tr, n)\n",
    "    plusDI  = 100 * EMA(pd.Series(plusDM, index=H.index), n) / atr\n",
    "    minusDI = 100 * EMA(pd.Series(minusDM, index=H.index), n) / atr\n",
    "    dx = 100 * ((plusDI - minusDI).abs() / (plusDI + minusDI))\n",
    "    return EMA(dx, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aroon(xH, xL, n=14):\n",
    "    def _aroon_up(series):\n",
    "        idx = series.argmax()\n",
    "        return ((n - (len(series)-1 - idx)) / n) * 100\n",
    "    def _aroon_dn(series):\n",
    "        idx = series.argmin()\n",
    "        return ((n - (len(series)-1 - idx)) / n) * 100\n",
    "    au = xH.rolling(n).apply(_aroon_up, raw=True)\n",
    "    ad = xL.rolling(n).apply(_aroon_dn, raw=True)\n",
    "    return pd.DataFrame({\"aroonUp\": au, \"aroonDn\": ad})\n",
    "\n",
    "def BBands(x, n=20, k=2):\n",
    "    m  = SMA(x, n)\n",
    "    sd = x.rolling(n).std()\n",
    "    return pd.DataFrame({\n",
    "        \"bb_up\":  m + k*sd,\n",
    "        \"bb_mid\": m,\n",
    "        \"bb_dn\":  m - k*sd\n",
    "    })\n",
    "\n",
    "def CCI(H, L, C, n=20):\n",
    "    TP = (H + L + C) / 3\n",
    "    M  = SMA(TP, n)\n",
    "    MD = TP.rolling(n).apply(lambda s: np.mean(np.abs(s - s.mean())), raw=True)\n",
    "    return (TP - M) / (0.015 * MD)\n",
    "\n",
    "def chaikin_volatility(H, L, n=10, ema_n=10):\n",
    "    hl = H - L\n",
    "    e1 = EMA(hl, ema_n)\n",
    "    e2 = e1.shift(n)\n",
    "    return (e1 - e2) / e2 * 100\n",
    "\n",
    "def CLV(H, L, C):\n",
    "    return ((C - L) - (H - C)) / (H - L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CMO(x, n=14):\n",
    "    diff = x.diff()\n",
    "    up = diff.where(diff>0, 0.0).rolling(n).sum()\n",
    "    dn = (-diff).where(diff<0, 0.0).rolling(n).sum()\n",
    "    return 100 * (up - dn) / (up + dn)\n",
    "\n",
    "def CTI(C, n=10):\n",
    "    m   = SMA(C, n)\n",
    "    dev = (C - m).abs()\n",
    "    return (C - m) / SMA(dev, n)\n",
    "\n",
    "def DonchianChannel(H, L, n=20):\n",
    "    up  = H.rolling(n).max()\n",
    "    dn  = L.rolling(n).min()\n",
    "    mid = (up + dn) / 2\n",
    "    return pd.DataFrame({\"dc_up\": up, \"dc_mid\": mid, \"dc_dn\": dn})\n",
    "\n",
    "def DPO(x, n=20):\n",
    "    m     = SMA(x, n)\n",
    "    shift = int(n/2 + 1)\n",
    "    return x.shift(shift) - m\n",
    "\n",
    "def DVI(C, n=14):\n",
    "    rng  = C.diff().abs()\n",
    "    prev = rng.shift(n)\n",
    "    dvi  = (rng - prev) / prev * 100\n",
    "    return dvi.where(prev != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMMA(x):\n",
    "    short = [3,5,8,10,12,15]\n",
    "    long  = [30,35,40,45,50,60]\n",
    "    df = {}\n",
    "    for i in short: df[f\"gmma_s{i}\"] = EMA(x, i)\n",
    "    for i in long:  df[f\"gmma_l{i}\"] = EMA(x, i)\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def KST(C, r1=10,r2=15,r3=20,r4=30,n1=10,n2=10,n3=10,n4=15):\n",
    "    roc1 = C.diff(r1)/C.shift(r1)\n",
    "    roc2 = C.diff(r2)/C.shift(r2)\n",
    "    roc3 = C.diff(r3)/C.shift(r3)\n",
    "    roc4 = C.diff(r4)/C.shift(r4)\n",
    "    kst    = (SMA(roc1,n1)*1 + SMA(roc2,n2)*2 +\n",
    "              SMA(roc3,n3)*3 + SMA(roc4,n4)*4)\n",
    "    signal = SMA(kst, 9)\n",
    "    return pd.DataFrame({\"kst\": kst, \"signal\": signal})\n",
    "\n",
    "def lags(H, L, C, n=1):\n",
    "    return C.diff(n)\n",
    "\n",
    "def MACD(C, n_fast=12, n_slow=26, n_sig=9):\n",
    "    fast = EMA(C, n_fast)\n",
    "    slow = EMA(C, n_slow)\n",
    "    macd = fast - slow\n",
    "    sig  = EMA(macd, n_sig)\n",
    "    hist = macd - sig\n",
    "    return pd.DataFrame({\"macd\": macd, \"signal\": sig, \"hist\": hist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PBands(C, n=20, pct=0.025):\n",
    "    m = SMA(C, n)\n",
    "    return pd.DataFrame({\n",
    "        \"pb_up\":  m * (1 + pct),\n",
    "        \"pb_mid\": m,\n",
    "        \"pb_dn\":  m * (1 - pct)\n",
    "    })\n",
    "\n",
    "def ROC(C, n=1):\n",
    "    return (C - C.shift(n)) / C.shift(n) * 100\n",
    "\n",
    "def momentum(C, n=1):\n",
    "    return C - C.shift(n)\n",
    "\n",
    "def RSI(C, n=14):\n",
    "    diff  = C.diff()\n",
    "    up    = diff.where(diff>0, 0.0)\n",
    "    dn    = -diff.where(diff<0,0.0)\n",
    "    ma_up = EMA(up, n)\n",
    "    ma_dn = EMA(dn, n)\n",
    "    rs    = ma_up / ma_dn\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def runSum(C, n=10):    return C.rolling(n).sum()\n",
    "def runMin(C, n=10):    return C.rolling(n).min()\n",
    "def runMax(C, n=10):    return C.rolling(n).max()\n",
    "def runMedian(C,n=10):  return C.rolling(n).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAR(H, L, af0=0.02, af_step=0.02, af_max=0.2):\n",
    "    up = True\n",
    "    af = af0\n",
    "    ep = H.iloc[0]\n",
    "    sar = [L.iloc[0]]\n",
    "    for i in range(1, len(H)):\n",
    "        prev = sar[-1]\n",
    "        if up:\n",
    "            candidate = prev + af*(ep - prev)\n",
    "            candidate = min(candidate, L.iloc[i-1], L.iloc[i])\n",
    "            if H.iloc[i] > ep:\n",
    "                ep = H.iloc[i]; af = min(af+af_step, af_max)\n",
    "            if L.iloc[i] < candidate:\n",
    "                up = False; sar.append(ep); ep = L.iloc[i]; af = af0\n",
    "            else:\n",
    "                sar.append(candidate)\n",
    "        else:\n",
    "            candidate = prev + af*(ep - prev)\n",
    "            candidate = max(candidate, H.iloc[i-1], H.iloc[i])\n",
    "            if L.iloc[i] < ep:\n",
    "                ep = L.iloc[i]; af = min(af+af_step, af_max)\n",
    "            if H.iloc[i] > candidate:\n",
    "                up = True; sar.append(ep); ep = H.iloc[i]; af = af0\n",
    "            else:\n",
    "                sar.append(candidate)\n",
    "    return pd.Series(sar, index=H.index)\n",
    "\n",
    "def volatility(H, L, C, n=10):\n",
    "    lr = np.log(C / C.shift(1))\n",
    "    return lr.rolling(n).std() * np.sqrt(252 * (390/(16*60)))\n",
    "\n",
    "def ultimate_oscillator(H, L, C, s1=7,s2=14,s3=28,w1=4,w2=2,w3=1):\n",
    "    bp  = C - np.minimum(L, C.shift(1))\n",
    "    tr  = np.maximum(H-L, np.maximum((H-C.shift(1)).abs(), (L-C.shift(1)).abs()))\n",
    "    avg1 = bp.rolling(s1).sum() / tr.rolling(s1).sum()\n",
    "    avg2 = bp.rolling(s2).sum() / tr.rolling(s2).sum()\n",
    "    avg3 = bp.rolling(s3).sum() / tr.rolling(s3).sum()\n",
    "    return 100 * (w1*avg1 + w2*avg2 + w3*avg3) / (w1+w2+w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VHF(C, n=28):\n",
    "    num = C.rolling(n).max() - C.rolling(n).min()\n",
    "    den = C.diff().abs().rolling(n).sum()\n",
    "    return num / den\n",
    "\n",
    "def williamsAD(H, L, C):\n",
    "    return ((C - L) - (H - C)) / (H - L)\n",
    "\n",
    "def WPR(H, L, C, n=14):\n",
    "    highest = H.rolling(n).max()\n",
    "    lowest  = L.rolling(n).min()\n",
    "    return (highest - C) / (highest - lowest) * -100\n",
    "\n",
    "def ZigZag(H, L, C, pct=0.05):\n",
    "    zz = [np.nan]*len(C)\n",
    "    last_pivot = C.iloc[0]\n",
    "    last_dir   = None\n",
    "    for i in range(1, len(C)):\n",
    "        change = (C.iloc[i] - last_pivot) / last_pivot\n",
    "        if last_dir is None:\n",
    "            if abs(change) > pct:\n",
    "                last_dir, last_pivot = np.sign(change), C.iloc[i]\n",
    "                zz[i] = last_pivot\n",
    "        else:\n",
    "            if (last_dir>0 and C.iloc[i]> last_pivot) or \\\n",
    "               (last_dir<0 and C.iloc[i]< last_pivot):\n",
    "                last_pivot = C.iloc[i]; zz[i] = last_pivot\n",
    "            elif abs(change) > pct:\n",
    "                last_dir, last_pivot = -last_dir, C.iloc[i]\n",
    "                zz[i] = last_pivot\n",
    "    return pd.Series(zz, index=C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRIX(C, n=9):\n",
    "    ema1 = EMA(C, n)\n",
    "    ema2 = EMA(ema1, n)\n",
    "    ema3 = EMA(ema2, n)\n",
    "    return ema3.pct_change() * 100\n",
    "\n",
    "def TDI(C, n_rsi=13, n_sig=2, n_bb=34, sd=1.618):\n",
    "    rsi    = RSI(C, n_rsi)\n",
    "    signal = SMA(rsi, n_sig)\n",
    "    bb     = BBands(rsi, n_bb, sd)\n",
    "    return pd.DataFrame({\n",
    "        \"rsi\":    rsi,\n",
    "        \"signal\": signal,\n",
    "        \"bb_up\":  bb[\"bb_up\"],\n",
    "        \"bb_mid\": bb[\"bb_mid\"],\n",
    "        \"bb_dn\":  bb[\"bb_dn\"]\n",
    "    })\n",
    "\n",
    "def SMI(H, L, C, n=14, n_fast=3, n_slow=25, n_sig=9):\n",
    "    m = (H.rolling(n).max() + L.rolling(n).min())/2\n",
    "    d = (H.rolling(n).max() - L.rolling(n).min())/2\n",
    "    num = EMA(EMA(C-m, n_fast), n_slow)\n",
    "    den = EMA(EMA(d,      n_fast), n_slow)\n",
    "    smi    = 100 * (num/den)\n",
    "    signal = EMA(smi, n_sig)\n",
    "    return pd.DataFrame({\"smi\": smi, \"signal\": signal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) —— Compute & aggregate “first-n-bar” daily averages —— #\n",
    "\n",
    "def compute_daily_averages(spx):\n",
    "    H, L, C, O = spx[\"High\"], spx[\"Low\"], spx[\"Close\"], spx[\"Open\"]\n",
    "    day_slices = []\n",
    "    for date, df in spx.groupby(\"Date\"):\n",
    "        idx = df.index\n",
    "        day_slices.append((idx[0], idx[-31]))\n",
    "    # precompute all indicators\n",
    "    inds = {\n",
    "        \"avg_ADX\":               ADX(H,L,C),\n",
    "        \"avg_aroonUp\":           aroon(H,L)[\"aroonUp\"],\n",
    "        \"avg_aroonDn\":           aroon(H,L)[\"aroonDn\"],\n",
    "        \"avg_ATR\":               ATR(H,L,C),\n",
    "        \"avg_BBands_up\":         BBands(C)[\"bb_up\"],\n",
    "        \"avg_BBands_mid\":        BBands(C)[\"bb_mid\"],\n",
    "        \"avg_BBands_dn\":         BBands(C)[\"bb_dn\"],\n",
    "        \"avg_CCI\":               CCI(H,L,C),\n",
    "        \"avg_chaikinVolatility\": chaikin_volatility(H,L),\n",
    "        \"avg_CLV\":               CLV(H,L,C),\n",
    "        \"avg_CMOClose\":          CMO(C),\n",
    "        \"avg_CTI\":               CTI(C),\n",
    "        \"avg_Donchian_up\":       DonchianChannel(H,L)[\"dc_up\"],\n",
    "        \"avg_Donchian_mid\":      DonchianChannel(H,L)[\"dc_mid\"],\n",
    "        \"avg_Donchian_dn\":       DonchianChannel(H,L)[\"dc_dn\"],\n",
    "        \"avg_DPOClose\":          DPO(C),\n",
    "        \"avg_DVIClose\":          DVI(C),\n",
    "        \"avg_GMMAClose\":         GMMA(C).mean(axis=1),\n",
    "        \"avg_KSTClose\":          KST(C)[\"kst\"],\n",
    "        \"avg_lagsClose\":         lags(H,L,C),\n",
    "        \"avg_MACD\":              MACD(C)[\"macd\"],\n",
    "        \"avg_PBands_up\":         PBands(C)[\"pb_up\"],\n",
    "        \"avg_PBands_mid\":        PBands(C)[\"pb_mid\"],\n",
    "        \"avg_PBands_dn\":         PBands(C)[\"pb_dn\"],\n",
    "        \"avg_ROCClose\":          ROC(C),\n",
    "        \"avg_momentumClose\":     momentum(C),\n",
    "        \"avg_RSIClose\":          RSI(C),\n",
    "        \"avg_runSum\":            runSum(C,10),\n",
    "        \"avg_runMin\":            runMin(C,10),\n",
    "        \"avg_runMax\":            runMax(C,10),\n",
    "        \"avg_runMedian\":         runMedian(C,10),\n",
    "        \"avg_SAR\":               SAR(H,L),\n",
    "        \"avg_SMAClose\":          SMA(C,20),\n",
    "        \"avg_EMAClose\":          EMA(C,20),\n",
    "        \"avg_DEMAClose\":         DEMA(C,20),\n",
    "        \"avg_SNR\":               VHF(C,28),\n",
    "        \"avg_SMI\":               SMI(H,L,C)[\"smi\"],\n",
    "        \"avg_TDI\":               TDI(C)[\"rsi\"],\n",
    "        \"avg_TRIX\":              TRIX(C),\n",
    "        \"avg_ultimateOscillator\":ultimate_oscillator(H,L,C),\n",
    "        \"avg_VHF\":               VHF(C),\n",
    "        \"avg_volatility\":        volatility(H,L,C),\n",
    "        \"avg_williamsAD\":        williamsAD(H,L,C),\n",
    "        \"avg_WPR\":               WPR(H,L,C),\n",
    "        \"avg_ZigZag\":            ZigZag(H,L,C)\n",
    "    }\n",
    "    data = {}\n",
    "    for name, series in inds.items():\n",
    "        if isinstance(series, pd.DataFrame):\n",
    "            for col in series.columns:\n",
    "                key = f\"{name}_{col}\"\n",
    "                data[key] = [\n",
    "                    series[col].loc[s:e].mean() for s,e in day_slices\n",
    "                ]\n",
    "        else:\n",
    "            data[name] = [\n",
    "                series.loc[s:e].mean() for s,e in day_slices\n",
    "            ]\n",
    "    return pd.DataFrame(data, index=y.index)\n",
    "\n",
    "# compute & save feature matrix\n",
    "features = compute_daily_averages(spx)\n",
    "features[\"Y\"] = y\n",
    "features.to_csv(FEATURE_CSV, index_label=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) —— Build allSet & Modeling —— #\n",
    "\n",
    "allset = features.dropna().reset_index(drop=True)\n",
    "allset.to_csv(ALLSET_CSV, index=False)\n",
    "\n",
    "# prepare data\n",
    "X   = allset.drop(\"Y\", axis=1).values\n",
    "y_  = allset[\"Y\"].values\n",
    "nx  = len(allset)\n",
    "train_n = int(np.floor(nx * 2/3))\n",
    "\n",
    "X_train, X_test = X[:train_n], X[train_n:]\n",
    "y_train, y_test = y_[:train_n], y_[train_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "def eval_model(X_tr, y_tr, X_te, y_te):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=-1)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    _ = cross_val_score(rf, X_tr, y_tr, cv=cv, scoring=\"accuracy\")\n",
    "    rf.fit(X_tr, y_tr)\n",
    "    y_pred = rf.predict(X_te)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "        \"precision\": precision_score(y_te, y_pred, pos_label=0),\n",
    "        \"recall\": recall_score(y_te, y_pred, pos_label=0),\n",
    "        \"f1\": f1_score(y_te, y_pred, pos_label=0),\n",
    "        \"cm\": confusion_matrix(y_te, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Accuracy  Precision    Recall        F1\n",
      "original  0.601923   0.547231  0.711864  0.618785\n",
      "under     0.623077   0.563694  0.750000  0.643636\n",
      "over      0.696154   0.683962  0.614407  0.647321\n",
      "smote     0.607692   0.552632  0.711864  0.622222\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# original\n",
    "results[\"original\"] = eval_model(X_train_s, y_train, X_test_s, y_test)\n",
    "# undersample\n",
    "rus = RandomUnderSampler(random_state=1)\n",
    "X_ru, y_ru = rus.fit_resample(X_train_s, y_train)\n",
    "results[\"under\"] = eval_model(X_ru, y_ru, X_test_s, y_test)\n",
    "# oversample\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_ro, y_ro = ros.fit_resample(X_train_s, y_train)\n",
    "results[\"over\"] = eval_model(X_ro, y_ro, X_test_s, y_test)\n",
    "# SMOTE\n",
    "sm = SMOTE(random_state=1)\n",
    "X_sm, y_sm = sm.fit_resample(X_train_s, y_train)\n",
    "results[\"smote\"] = eval_model(X_sm, y_sm, X_test_s, y_test)\n",
    "\n",
    "# summarize\n",
    "comp = pd.DataFrame({\n",
    "    m: {\n",
    "        \"Accuracy\":  results[m][\"accuracy\"],\n",
    "        \"Precision\": results[m][\"precision\"],\n",
    "        \"Recall\":    results[m][\"recall\"],\n",
    "        \"F1\":        results[m][\"f1\"]\n",
    "    } for m in results\n",
    "}).T\n",
    "\n",
    "print(comp)\n",
    "\n",
    "comp.to_csv(\"../csvfiles_new/comparison_360.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
