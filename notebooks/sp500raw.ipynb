{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting S&P 500 Price Movements with Machine Learning\n",
    "\n",
    "This notebook aims to forecast daily price movements of the **S&P 500 index** using machine learning classifier **Random Forest**, combined with **resampling techniques** to address class imbalance (e.g., **SMOTE**, undersampling).\n",
    "\n",
    "---\n",
    "\n",
    "### Forecasting Objective\n",
    "\n",
    "We frame this as a **binary classification problem**, where the goal is to predict whether the S&P 500 will close **above or below its intraday average** based on early trading data.\n",
    "\n",
    "For each trading day:\n",
    "\n",
    "1. Extract all 1-minute price candles from market open until **30 minutes before close**.\n",
    "2. Compute the **average price** over this period.\n",
    "3. Compare it with the **closing price of the last 1-minute candle** of the day:\n",
    "   - If the final price is **above** the average → target = `1`\n",
    "   - If it is **below** the average → target = `0`\n",
    "\n",
    "This formulation allows the model to generate **intra-day trading signals** using only early and mid-day price data.\n",
    "\n",
    "---\n",
    "\n",
    "### Background and Translation Notes\n",
    "\n",
    "This notebook is a **Python translation** of the original R-based methodology from [this GitHub repository](https://github.com/LuZhang907/RandomForest).  \n",
    "The original code leverages **ROSE (Random OverSampling Examples)** to address class imbalance by generating synthetic samples with added noise. However, as ROSE is not natively available in Python's ecosystem, we focus on **SMOTE (Synthetic Minority Over-sampling Technique)**, which creates new samples through interpolation between existing minority instances.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "To reproduce this project, please ensure the following dependencies are installed:\n",
    "\n",
    "- Python\n",
    "- pandas  \n",
    "- numpy  \n",
    "- scikit-learn \n",
    "- imbalanced-learn\n",
    "- pytz\n",
    "- PyWavelets \n",
    "\n",
    "You can install them with:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Or by running:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn imbalanced-learn pytz PyWavelets\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "This section loads all required libraries for data manipulation, model training, evaluation, and resampling. Ensure that all dependencies listed in the environment section are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import time\n",
    "from pytz import timezone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Aggregation\n",
    "This section loads the raw CSV files (presumably minute-level S&P 500 stock data), aggregates them, and converts them into a usable dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1d219c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) —— Data Import & Preprocessing —— #\n",
    "\n",
    "DATA_DIR = \"../SPX\"\n",
    "OUTPUT_FEATURE_CSV = \"../csvfiles_python/features_360_raw.csv\"\n",
    "OUTPUT_ALLSET_CSV  = \"../csvfiles_python/allSet_360_raw.csv\"\n",
    "\n",
    "def load_data(path):\n",
    "    files = glob.glob(os.path.join(path, \"*.txt\"))\n",
    "    dfs = []\n",
    "    for f in sorted(files):\n",
    "        df = pd.read_csv(f, names=[\"DateTime\",\"Open\",\"High\",\"Low\",\"Close\"],\n",
    "                         sep=\",\", parse_dates=[\"DateTime\"])\n",
    "        dfs.append(df)\n",
    "    spx = pd.concat(dfs, ignore_index=True)\n",
    "    # localize to EST\n",
    "    spx[\"DateTime\"] = spx[\"DateTime\"].dt.tz_localize(timezone(\"US/Eastern\"))\n",
    "    spx.set_index(\"DateTime\", inplace=True)\n",
    "    # filter trading hours 9:30–16:00\n",
    "    return spx.between_time(\"09:30\",\"16:00\")\n",
    "\n",
    "spx = load_data(DATA_DIR)\n",
    "\n",
    "# Drop duplicates\n",
    "spx = spx[~spx.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Construction\n",
    "A binary classification target is defined based on future price movement (e.g., 1 if the price increases, 0 if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daf95c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101545/266240317.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  avgP = groups.apply(lambda df: df[\"Close\"].iloc[:-30].mean())\n"
     ]
    }
   ],
   "source": [
    "# 2) —— Create daily labels —— #\n",
    "\n",
    "# group by calendar date\n",
    "spx[\"Date\"] = spx.index.date\n",
    "groups = spx.groupby(\"Date\")\n",
    "\n",
    "# last-minute close price per day\n",
    "lmP = groups[\"Close\"].last()\n",
    "\n",
    "# average price from start to (end − 30 mins)\n",
    "avgP = groups.apply(lambda df: df[\"Close\"].iloc[:-30].mean())\n",
    "\n",
    "# binary label: 1 if avgP < lmP else 0\n",
    "y = (avgP < lmP).astype(int)\n",
    "y.name = \"Y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "Here, new features are created for modeling purposes. Examples include previous price differences or multiple technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87981975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) —— Technical‐Indicator Functions —— #\n",
    "\n",
    "# Basic EMAs & SMAs\n",
    "def SMA(x, n):\n",
    "    return x.rolling(n, min_periods=n).mean()\n",
    "\n",
    "def EMA(x, n):\n",
    "    return x.ewm(span=n, adjust=False).mean()\n",
    "\n",
    "def DEMA(x, n):\n",
    "    e = EMA(x, n)\n",
    "    return 2*e - EMA(e, n)\n",
    "\n",
    "# True Range & ATR\n",
    "def ATR(H, L, C, n=14):\n",
    "    prevC = C.shift(1)\n",
    "    tr = pd.concat([\n",
    "        H - L,\n",
    "        (H - prevC).abs(),\n",
    "        (L - prevC).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return EMA(tr, n)\n",
    "\n",
    "# Average Directional Index (ADX)\n",
    "def ADX(H, L, C, n=14):\n",
    "    up = H.diff()\n",
    "    down = -L.diff()\n",
    "    plusDM  = np.where((up>down)&(up>0), up, 0.0)\n",
    "    minusDM = np.where((down>up)&(down>0), down, 0.0)\n",
    "    tr = pd.concat([\n",
    "        H-L,\n",
    "        (H - C.shift(1)).abs(),\n",
    "        (L - C.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    atr = EMA(tr, n)\n",
    "    plusDI  = 100 * EMA(pd.Series(plusDM, index=H.index), n) / atr\n",
    "    minusDI = 100 * EMA(pd.Series(minusDM, index=H.index), n) / atr\n",
    "    dx = 100 * ( (plusDI - minusDI).abs() / (plusDI + minusDI) )\n",
    "    return EMA(dx, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d90950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aroon\n",
    "def aroon(xH, xL, n=14):\n",
    "    def _aroon_up(series):\n",
    "        idx = series.argmax()\n",
    "        return ((n - (len(series)-1 - idx)) / n) * 100\n",
    "    def _aroon_dn(series):\n",
    "        idx = series.argmin()\n",
    "        return ((n - (len(series)-1 - idx)) / n) * 100\n",
    "\n",
    "    au = xH.rolling(n).apply(_aroon_up, raw=True)\n",
    "    ad = xL.rolling(n).apply(_aroon_dn, raw=True)\n",
    "    return pd.DataFrame({\"aroonUp\": au, \"aroonDn\": ad})\n",
    "\n",
    "# Bollinger Bands\n",
    "def BBands(x, n=20, k=2):\n",
    "    m = SMA(x, n)\n",
    "    sd = x.rolling(n).std()\n",
    "    return pd.DataFrame({\n",
    "        \"bb_up\":   m + k*sd,\n",
    "        \"bb_mid\":  m,\n",
    "        \"bb_low\":  m - k*sd\n",
    "    })\n",
    "\n",
    "# Commodity Channel Index\n",
    "def CCI(H, L, C, n=20):\n",
    "    TP = (H + L + C) / 3\n",
    "    M  = SMA(TP, n)\n",
    "    MD = TP.rolling(n).apply(lambda s: np.mean(np.abs(s - s.mean())), raw=True)\n",
    "    return (TP - M) / (0.015 * MD)\n",
    "\n",
    "# Chaikin Volatility\n",
    "def chaikin_volatility(H, L, n=10, ema_n=10):\n",
    "    hl = H - L\n",
    "    e1 = EMA(hl, ema_n)\n",
    "    e2 = e1.shift(n)\n",
    "    return (e1 - e2) / e2 * 100\n",
    "\n",
    "# Close Location Value\n",
    "def CLV(H, L, C):\n",
    "    return ((C - L) - (H - C)) / (H - L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf02e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chande Momentum Oscillator\n",
    "def CMO(x, n=14):\n",
    "    diff = x.diff()\n",
    "    up = diff.where(diff>0, 0.0).rolling(n).sum()\n",
    "    dn = (-diff).where(diff<0, 0.0).rolling(n).sum()\n",
    "    return 100 * (up - dn) / (up + dn)\n",
    "\n",
    "# Centre of Gravity Indicator (CTI)\n",
    "#  TTR’s CTI uses n=10 by default: CTI = (C - SMA(C,n)) / SMA(abs(C - SMA(C,n)),n)\n",
    "def CTI(C, n=10):\n",
    "    m = SMA(C, n)\n",
    "    dev = (C - m).abs()\n",
    "    return (C - m) / SMA(dev, n)\n",
    "\n",
    "# Donchian Channel\n",
    "def DonchianChannel(H, L, n=20):\n",
    "    up = H.rolling(n).max()\n",
    "    dn = L.rolling(n).min()\n",
    "    mid = (up + dn) / 2\n",
    "    return pd.DataFrame({\"dc_up\": up, \"dc_mid\": mid, \"dc_dn\": dn})\n",
    "\n",
    "# Detrended Price Oscillator\n",
    "def DPO(x, n=20):\n",
    "    m = SMA(x, n)\n",
    "    shift = int(n/2 + 1)\n",
    "    return x.shift(shift) - m\n",
    "\n",
    "# Directional Volatility Index (DVI)\n",
    "#  TTR’s DVI default n=14: DVI = ROC of range\n",
    "def DVI(C, n=14):\n",
    "    rng = C.diff().abs()\n",
    "    prev = rng.shift(n)\n",
    "\n",
    "    # safe division: if prev==0 → NaN, else compute as usual\n",
    "    dvi = (rng - prev) / prev * 100\n",
    "    dvi = dvi.where(prev != 0)   # sets those inf entries to NaN\n",
    "\n",
    "    return dvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30253a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guppy Multiple Moving Average\n",
    "def GMMA(x):\n",
    "    short = [3,5,8,10,12,15]\n",
    "    long  = [30,35,40,45,50,60]\n",
    "    df = {}\n",
    "    for i in short: df[f\"gmma_s{i}\"] = EMA(x, i)\n",
    "    for i in long:  df[f\"gmma_l{i}\"] = EMA(x, i)\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "# Know Sure Thing\n",
    "def KST(C, r1=10, r2=15, r3=20, r4=30, \n",
    "        n1=10, n2=10, n3=10, n4=15):\n",
    "    roc1 = C.diff(r1)/C.shift(r1)\n",
    "    roc2 = C.diff(r2)/C.shift(r2)\n",
    "    roc3 = C.diff(r3)/C.shift(r3)\n",
    "    roc4 = C.diff(r4)/C.shift(r4)\n",
    "    kst = (SMA(roc1, n1)*1 +\n",
    "           SMA(roc2, n2)*2 +\n",
    "           SMA(roc3, n3)*3 +\n",
    "           SMA(roc4, n4)*4)\n",
    "    signal = SMA(kst, 9)\n",
    "    return pd.DataFrame({\"kst\": kst, \"signal\": signal})\n",
    "\n",
    "# Lagged differences\n",
    "def lags(H, L, C, n=1):\n",
    "    # show lagged close differences by default\n",
    "    return C.diff(n)\n",
    "\n",
    "# MACD\n",
    "def MACD(C, n_fast=12, n_slow=26, n_sig=9):\n",
    "    fast = EMA(C, n_fast)\n",
    "    slow = EMA(C, n_slow)\n",
    "    macd = fast - slow\n",
    "    sig  = EMA(macd, n_sig)\n",
    "    hist = macd - sig\n",
    "    return pd.DataFrame({\"macd\": macd, \"signal\": sig, \"hist\": hist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26d4a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Bands (percent)\n",
    "def PBands(C, n=20, pct=0.025):\n",
    "    m = SMA(C, n)\n",
    "    return pd.DataFrame({\n",
    "        \"pb_up\": m*(1+pct),\n",
    "        \"pb_mid\": m,\n",
    "        \"pb_dn\": m*(1-pct)\n",
    "    })\n",
    "\n",
    "# Rate of Change\n",
    "def ROC(C, n=1):\n",
    "    return (C - C.shift(n)) / C.shift(n) * 100\n",
    "\n",
    "# Momentum\n",
    "def momentum(C, n=1):\n",
    "    return C - C.shift(n)\n",
    "\n",
    "# Relative Strength Index\n",
    "def RSI(C, n=14):\n",
    "    diff = C.diff()\n",
    "    up = diff.where(diff>0, 0.0)\n",
    "    dn = -diff.where(diff<0, 0.0)\n",
    "    ma_up = EMA(up, n)\n",
    "    ma_dn = EMA(dn, n)\n",
    "    rs = ma_up / ma_dn\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "# Running stats\n",
    "def runSum(C, n=10):    return C.rolling(n).sum()\n",
    "def runMin(C, n=10):    return C.rolling(n).min()\n",
    "def runMax(C, n=10):    return C.rolling(n).max()\n",
    "def runMedian(C,n=10):  return C.rolling(n).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ccfce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parabolic SAR\n",
    "def SAR(H, L, af0=0.02, af_step=0.02, af_max=0.2):\n",
    "    up = True\n",
    "    af = af0\n",
    "    ep = H[0]\n",
    "    sar = [L.iloc[0]]\n",
    "    for i in range(1, len(H)):\n",
    "        prev_sar = sar[-1]\n",
    "        if up:\n",
    "            new_sar = prev_sar + af*(ep - prev_sar)\n",
    "            new_sar = min(new_sar, L.iloc[i-1], L.iloc[i])\n",
    "            if H.iloc[i] > ep:\n",
    "                ep = H.iloc[i]; af = min(af+af_step, af_max)\n",
    "            if L.iloc[i] < new_sar:\n",
    "                up = False; sar.append(ep); ep = L.iloc[i]; af = af0\n",
    "            else:\n",
    "                sar.append(new_sar)\n",
    "        else:\n",
    "            new_sar = prev_sar + af*(ep - prev_sar)\n",
    "            new_sar = max(new_sar, H.iloc[i-1], H.iloc[i])\n",
    "            if L.iloc[i] < ep:\n",
    "                ep = L.iloc[i]; af = min(af+af_step, af_max)\n",
    "            if H.iloc[i] > new_sar:\n",
    "                up = True; sar.append(ep); ep = H.iloc[i]; af = af0\n",
    "            else:\n",
    "                sar.append(new_sar)\n",
    "    return pd.Series(sar, index=H.index)\n",
    "\n",
    "# Volatility (rolling stdev of log‐returns)\n",
    "def volatility(H, L, C, n=10):\n",
    "    lr = np.log(C / C.shift(1))\n",
    "    return lr.rolling(n).std() * np.sqrt(252* (390/ (16*60)))  # annualize on minute data\n",
    "\n",
    "# Ultimate Oscillator\n",
    "def ultimate_oscillator(H, L, C, s1=7, s2=14, s3=28, w1=4, w2=2, w3=1):\n",
    "    bp = C - np.minimum(L, C.shift(1))\n",
    "    tr = np.maximum(H - L, np.maximum((H-C.shift(1)).abs(), (L-C.shift(1)).abs()))\n",
    "    avg1 = bp.rolling(s1).sum() / tr.rolling(s1).sum()\n",
    "    avg2 = bp.rolling(s2).sum() / tr.rolling(s2).sum()\n",
    "    avg3 = bp.rolling(s3).sum() / tr.rolling(s3).sum()\n",
    "    uo = 100 * (w1*avg1 + w2*avg2 + w3*avg3) / (w1+w2+w3)\n",
    "    return uo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4de2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical–Horizontal Filter\n",
    "def VHF(C, n=28):\n",
    "    num = C.rolling(n).max() - C.rolling(n).min()\n",
    "    den = C.diff().abs().rolling(n).sum()\n",
    "    return num / den\n",
    "\n",
    "# Williams Accumulation/Distribution\n",
    "def williamsAD(H, L, C):\n",
    "    clv = ((C - L) - (H - C)) / (H - L)\n",
    "    return clv  # as in TTR\n",
    "\n",
    "# Williams %R\n",
    "def WPR(H, L, C, n=14):\n",
    "    highest = H.rolling(n).max()\n",
    "    lowest = L.rolling(n).min()\n",
    "    return (highest - C) / (highest - lowest) * -100\n",
    "\n",
    "# ZigZag\n",
    "def ZigZag(H, L, C, pct=0.05):\n",
    "    zz = [np.nan] * len(C)\n",
    "    last_pivot = C.iloc[0]\n",
    "    last_dir = None\n",
    "\n",
    "    for i in range(1, len(C)):\n",
    "        change = (C.iloc[i] - last_pivot) / last_pivot\n",
    "        if last_dir is None:\n",
    "            if abs(change) > pct:\n",
    "                last_dir = np.sign(change)\n",
    "                last_pivot = C.iloc[i]\n",
    "                zz[i] = last_pivot\n",
    "        else:\n",
    "            if last_dir > 0 and C.iloc[i] > last_pivot:\n",
    "                last_pivot = C.iloc[i]\n",
    "                zz[i] = last_pivot\n",
    "            elif last_dir < 0 and C.iloc[i] < last_pivot:\n",
    "                last_pivot = C.iloc[i]\n",
    "                zz[i] = last_pivot\n",
    "            # reversal?\n",
    "            elif (last_dir > 0 and (C.iloc[i] - last_pivot) / last_pivot < -pct) or \\\n",
    "                 (last_dir < 0 and (C.iloc[i] - last_pivot) / last_pivot > pct):\n",
    "                last_dir *= -1\n",
    "                last_pivot = C.iloc[i]\n",
    "                zz[i] = last_pivot\n",
    "\n",
    "    return pd.Series(zz, index=C.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82fc4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Triple Exponential Average oscillator (TRIX) ---\n",
    "def TRIX(C, n=9):\n",
    "    ema1 = EMA(C, n)\n",
    "    ema2 = EMA(ema1, n)\n",
    "    ema3 = EMA(ema2, n)\n",
    "    trix = ema3.pct_change() * 100\n",
    "    return trix\n",
    "\n",
    "# --- Traders Dynamic Index (TDI) ---\n",
    "def TDI(C, n_rsi=13, n_sig=2, n_bb=34, sd=1.618):\n",
    "    rsi = RSI(C, n_rsi)\n",
    "    signal = SMA(rsi, n_sig)\n",
    "    bb    = BBands(rsi, n_bb, sd)\n",
    "    return pd.DataFrame({\n",
    "        \"rsi\":    rsi,\n",
    "        \"signal\": signal,\n",
    "        \"bb_up\":  bb[\"bb_up\"],\n",
    "        \"bb_mid\": bb[\"bb_mid\"],\n",
    "        \"bb_dn\":  bb[\"bb_low\"]\n",
    "    })\n",
    "\n",
    "# --- Stochastic Momentum Index (SMI) ---\n",
    "def SMI(H, L, C, n=14, n_fast=3, n_slow=25, n_sig=9):\n",
    "    \n",
    "    # median price and half‐range\n",
    "    m = (H.rolling(n).max() + L.rolling(n).min()) / 2\n",
    "    d = (H.rolling(n).max() - L.rolling(n).min()) / 2\n",
    "\n",
    "    # double‐smoothed numerator and denominator\n",
    "    num = EMA(EMA(C - m, n_fast), n_slow)\n",
    "    den = EMA(EMA(d,       n_fast), n_slow)\n",
    "\n",
    "    smi    = 100 * (num / den)\n",
    "    signal = EMA(smi, n_sig)\n",
    "\n",
    "    return pd.DataFrame({\"smi\": smi, \"signal\": signal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "081d2952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101545/3631590232.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ep = H[0]\n"
     ]
    }
   ],
   "source": [
    "# 4) —— Compute & aggregate “first-n-bar” daily averages —— #\n",
    "\n",
    "def compute_daily_averages(spx):\n",
    "    H, L, C, O = spx[\"High\"], spx[\"Low\"], spx[\"Close\"], spx[\"Open\"]\n",
    "    n_days = len(y)\n",
    "    # indices per day\n",
    "    day_slices = []\n",
    "    for date, df in spx.groupby(\"Date\"):\n",
    "        arr = df.index\n",
    "        day_slices.append((arr[0], arr[-31]))  # from first bar up to bar-30\n",
    "    \n",
    "    # precompute all indicator series\n",
    "    inds = {\n",
    "        \"avg_ADX\":               ADX(H,L,C),\n",
    "        \"avg_aroonUp\":           aroon(H,L)[\"aroonUp\"],\n",
    "        \"avg_aroonDn\":           aroon(H,L)[\"aroonDn\"],\n",
    "        \"avg_ATR\":               ATR(H,L,C),\n",
    "        \"avg_BBands_up\":         BBands(C)[\"bb_up\"],\n",
    "        \"avg_BBands_mid\":        BBands(C)[\"bb_mid\"],\n",
    "        \"avg_BBands_dn\":         BBands(C)[\"bb_low\"],\n",
    "        \"avg_CCI\":               CCI(H,L,C),\n",
    "        \"avg_chaikinVolatility\": chaikin_volatility(H,L),\n",
    "        \"avg_CLV\":               CLV(H,L,C),\n",
    "        \"avg_CMOClose\":          CMO(C),\n",
    "        \"avg_CTI\":               CTI(C),\n",
    "        \"avg_Donchian_up\":       DonchianChannel(H,L)[\"dc_up\"],\n",
    "        \"avg_Donchian_mid\":      DonchianChannel(H,L)[\"dc_mid\"],\n",
    "        \"avg_Donchian_dn\":       DonchianChannel(H,L)[\"dc_dn\"],\n",
    "        \"avg_DPOClose\":          DPO(C),\n",
    "        \"avg_DVIClose\":          DVI(C),\n",
    "        \"avg_GMMAClose\":         GMMA(C).mean(axis=1),\n",
    "        \"avg_KSTClose\":          KST(C)[\"kst\"],\n",
    "        \"avg_lagsClose\":         lags(H,L,C),\n",
    "        \"avg_MACD\":              MACD(C)[\"macd\"],\n",
    "        \"avg_PBands_up\":         PBands(C)[\"pb_up\"],\n",
    "        \"avg_PBands_mid\":        PBands(C)[\"pb_mid\"],\n",
    "        \"avg_PBands_dn\":         PBands(C)[\"pb_dn\"],\n",
    "        \"avg_ROCClose\":          ROC(C),\n",
    "        \"avg_momentumClose\":     momentum(C),\n",
    "        \"avg_RSIClose\":          RSI(C),\n",
    "        \"avg_runSum\":            runSum(C, 10),\n",
    "        \"avg_runMin\":            runMin(C, 10),\n",
    "        \"avg_runMax\":            runMax(C, 10),\n",
    "        \"avg_runMedian\":         runMedian(C, 10),\n",
    "        \"avg_SAR\":               SAR(H,L),\n",
    "        \"avg_SMAClose\":          SMA(C, 20),\n",
    "        \"avg_EMAClose\":          EMA(C, 20),\n",
    "        \"avg_DEMAClose\":         DEMA(C, 20),\n",
    "        \"avg_SNR\":               VHF(C,28),   # approximate SNR via VHF\n",
    "        \"avg_SMI\":               SMI(H,L,C),\n",
    "        \"avg_TDI\":               TDI(C),    \n",
    "        \"avg_TRIX\":              TRIX(C),   \n",
    "        \"avg_ultimateOscillator\":ultimate_oscillator(H,L,C),\n",
    "        \"avg_VHF\":               VHF(C),\n",
    "        \"avg_volatility\":        volatility(H,L,C),\n",
    "        \"avg_williamsAD\":        williamsAD(H,L,C),\n",
    "        \"avg_WPR\":               WPR(H,L,C),\n",
    "        \"avg_ZigZag\":            ZigZag(H,L,C)\n",
    "    }\n",
    "\n",
    "    data = {}\n",
    "    for name, series in inds.items():\n",
    "        # skip unimplemented features\n",
    "        if series is None: \n",
    "            continue\n",
    "\n",
    "        if isinstance(series, pd.DataFrame):\n",
    "            # for each column in the DataFrame, make a separate feature\n",
    "            for col in series.columns:\n",
    "                key = f\"{name}_{col}\"\n",
    "                data[key] = [\n",
    "                    series[col].loc[start:end].mean()\n",
    "                    for start, end in day_slices\n",
    "                ]\n",
    "        else:\n",
    "            data[name] = [\n",
    "                series.loc[start:end].mean()\n",
    "                for start, end in day_slices\n",
    "            ]\n",
    "\n",
    "    return pd.DataFrame(data, index=y.index)\n",
    "\n",
    "features = compute_daily_averages(spx)\n",
    "features[\"Y\"] = y\n",
    "features.to_csv(OUTPUT_FEATURE_CSV, index_label=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db424a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) —— Train/Test split & Modeling —— #\n",
    "\n",
    "# drop any rows with NA\n",
    "allset = features.dropna().reset_index(drop=True)\n",
    "allset.to_csv(OUTPUT_ALLSET_CSV, index=False)\n",
    "\n",
    "X = allset.drop(\"Y\", axis=1).values\n",
    "y_ = allset[\"Y\"].values\n",
    "nx = len(allset)\n",
    "train_n = int(np.floor(nx * 2/3))\n",
    "\n",
    "X_train, X_test = X[:train_n], X[train_n:]\n",
    "y_train, y_test = y_[:train_n], y_[train_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b33b1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "def eval_model(X_tr, y_tr, X_te, y_te):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=-1)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    scores = cross_val_score(rf, X_tr, y_tr, cv=cv, scoring=\"accuracy\")\n",
    "    rf.fit(X_tr, y_tr)\n",
    "    y_pred = rf.predict(X_te)\n",
    "    cm = confusion_matrix(y_te, y_pred)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "        \"precision\": precision_score(y_te, y_pred, pos_label=0),\n",
    "        \"recall\": recall_score(y_te, y_pred, pos_label=0),\n",
    "        \"f1\": f1_score(y_te, y_pred, pos_label=0),\n",
    "        \"cm\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resampling Techniques & Model Training\n",
    "We apply methods such as SMOTE, random oversampling, and undersampling to address class imbalance in the dataset.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic samples of the minority class by interpolating between exsisting, random oversampling duplicates existing samples from the minority class, while undersampling reduces the majority class to balance the dataset.\n",
    "\n",
    "A Random Forest Classifier is trained using k-fold cross-validation. The performance is evaluated on key metrics: accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae3436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Accuracy  Precision    Recall        F1\n",
      "original  0.737789   0.709302  0.701149  0.705202\n",
      "under     0.709512   0.654822  0.741379  0.695418\n",
      "over      0.706941   0.678571  0.655172  0.666667\n",
      "smote     0.727506   0.712500  0.655172  0.682635\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "results = {}\n",
    "results[\"original\"] = eval_model(X_train_s, y_train, X_test_s, y_test)\n",
    "\n",
    "# undersample\n",
    "rus = RandomUnderSampler(random_state=1)\n",
    "X_ru, y_ru = rus.fit_resample(X_train_s, y_train)\n",
    "results[\"under\"] = eval_model(X_ru, y_ru, X_test_s, y_test)\n",
    "\n",
    "# oversample\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_ro, y_ro = ros.fit_resample(X_train_s, y_train)\n",
    "results[\"over\"] = eval_model(X_ro, y_ro, X_test_s, y_test)\n",
    "\n",
    "# SMOTE\n",
    "sm = SMOTE(random_state=1)\n",
    "X_sm, y_sm = sm.fit_resample(X_train_s, y_train)\n",
    "results[\"smote\"] = eval_model(X_sm, y_sm, X_test_s, y_test)\n",
    "\n",
    "# print comparison\n",
    "comp = pd.DataFrame({\n",
    "    m: {\n",
    "        \"Accuracy\": results[m][\"accuracy\"],\n",
    "        \"Precision\": results[m][\"precision\"],\n",
    "        \"Recall\": results[m][\"recall\"],\n",
    "        \"F1\": results[m][\"f1\"]\n",
    "    } for m in results\n",
    "}).T\n",
    "print(comp)\n",
    "\n",
    "comp.to_csv(\"../csvfiles_python/comparison_360_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Results (Short Version)\n",
    "Among all sampling methods tested, the original dataset produced the best overall performance in terms of accuracy and F1 score, suggesting that resampling was not strictly necessary. SMOTE offered a balanced improvement in recall without sacrificing much precision, making it a good alternative if identifying positive signals is a priority. Undersampling achieved the highest recall but at the cost of precision, while oversampling showed limited benefit in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Notes\n",
    "Further improvement could involve using more advanced models (e.g., XGBoost) or time-series-specific approaches (e.g., LSTM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
